# NEURO-OLLAMA

Servicio Ollama independiente para el ecosistema NEURO-OS.

## üìã Descripci√≥n

Este repositorio contiene la configuraci√≥n Docker para ejecutar Ollama como servicio independiente, separado del repositorio principal de NEURO-OS. Esto permite:

- Ejecutar Ollama en una m√°quina dedicada con GPU
- Compartir el servicio entre m√∫ltiples instancias de NEURO-OS
- Gestionar modelos de forma independiente
- Actualizar Ollama sin afectar otros servicios

## üöÄ Inicio R√°pido

```bash
# 1. Clonar el repositorio
git clone https://github.com/tu-usuario/neuro-ollama.git
cd neuro-ollama

# 2. Iniciar Ollama
docker compose up -d

# 3. Instalar modelos requeridos
./scripts/setup.sh

# 4. Verificar que funciona
curl http://localhost:11434/api/tags
```

## üì¶ Modelos

### Modelos Instalados

| Modelo | VRAM | Uso |
|--------|------|-----|
| `glm4:9b-chat-q8_0` | ~9 GB | Modelo principal con tools |
| `nomic-embed-text` | ~500 MB | B√∫squeda sem√°ntica |

### Gu√≠a de Modelos Gratuitos de Ollama

Esta tabla muestra los modelos m√°s populares disponibles en Ollama y sus capacidades:

> **Nota sobre Tools/AI Coding**: La columna "üõ†Ô∏è Tools" indica compatibilidad con tool calling.
> - ‚úÖ = Soporta herramientas (compatible con asistentes de c√≥digo que usan tools)
> - ‚ùå = El modelo NO soporta tools en absoluto
> 
> *√öltima verificaci√≥n: Enero 2026 con Ollama 0.13.5*

#### Modelos Peque√±os (1-4 GB VRAM)

| Modelo | Params | VRAM | üõ†Ô∏è Tools | üëÅÔ∏è Visi√≥n | üíª C√≥digo | üåê Espa√±ol | Notas |
|--------|--------|------|----------|-----------|-----------|------------|-------|
| `qwen2.5:1.5b` | 1.5B | ~1 GB | ‚úÖ | ‚ùå | ‚≠ê‚≠ê | ‚≠ê‚≠ê | Ultra ligero |
| `qwen2.5:3b` | 3B | ~2 GB | ‚úÖ | ‚ùå | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | Buen balance |
| `gemma2:2b` | 2B | ~1.5 GB | ‚ùå | ‚ùå | ‚≠ê‚≠ê | ‚≠ê‚≠ê | Google, r√°pido |
| `phi3:mini` | 3.8B | ~2.5 GB | ‚ùå | ‚ùå | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | Microsoft |
| `llama3.2:3b` | 3B | ~2 GB | ‚úÖ | ‚ùå | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | Meta, vers√°til |
| `ministral-3:3b` | 3B | ~3 GB | ‚úÖ | ‚úÖ | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | **‚úÖ Recomendado Light** |

#### Modelos Medianos (4-8 GB VRAM)

| Modelo | Params | VRAM | üõ†Ô∏è Tools | üëÅÔ∏è Visi√≥n | üíª C√≥digo | üåê Espa√±ol | Notas |
|--------|--------|------|----------|-----------|-----------|------------|-------|
| `mistral:7b` | 7B | ~4 GB | ‚úÖ | ‚ùå | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | Cl√°sico, estable |
| `llama3.1:8b` | 8B | ~5 GB | ‚úÖ | ‚ùå | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | Meta, muy capaz |
| `gemma2:9b` | 9B | ~6 GB | ‚ùå | ‚ùå | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | Google, preciso |
| `qwen2.5:7b` | 7B | ~5 GB | ‚úÖ | ‚ùå | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Excelente en espa√±ol |
| `ministral-3:8b` | 8B | ~6 GB | ‚úÖ | ‚úÖ | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | Con visi√≥n |
| `deepseek-coder:6.7b` | 6.7B | ~4 GB | ‚ùå | ‚ùå | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | C√≥digo (sin tools) |

#### Modelos Grandes (8-16 GB VRAM)

| Modelo | Params | VRAM | üõ†Ô∏è Tools | üëÅÔ∏è Visi√≥n | üíª C√≥digo | üåê Espa√±ol | Notas |
|--------|--------|------|----------|-----------|-----------|------------|-------|
| `qwen2.5:14b` | 14B | ~9 GB | ‚úÖ | ‚ùå | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Muy capaz |
| `glm4:9b-chat-q8_0` | 9B | ~9 GB | ‚úÖ | ‚ùå | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | **‚úÖ Recomendado - GLM-4 Q8** |
| `llama3.1:70b-q4` | 70B | ~40 GB | ‚úÖ | ‚ùå | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | Necesita mucha VRAM |
| `codellama:13b` | 13B | ~8 GB | ‚ùå | ‚ùå | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | Solo c√≥digo |
| `mixtral:8x7b` | 47B | ~26 GB | ‚úÖ | ‚ùå | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | MoE (mucha VRAM) |
| `llava:13b` | 13B | ~10 GB | ‚ùå | ‚úÖ | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | Solo visi√≥n |
| `deepseek-coder-v2:16b` | 16B | ~10 GB | ‚ùå | ‚ùå | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | C√≥digo (sin tools) |

#### Modelos de Embeddings

| Modelo | Dimensiones | VRAM | Uso |
|--------|-------------|------|-----|
| `nomic-embed-text` | 768 | ~500 MB | **Recomendado** - B√∫squeda sem√°ntica general |
| `mxbai-embed-large` | 1024 | ~700 MB | Mayor precisi√≥n, m√°s lento |
| `all-minilm` | 384 | ~100 MB | Ultra ligero, b√°sico |
| `snowflake-arctic-embed` | 1024 | ~700 MB | Bueno para RAG |

#### Leyenda

| S√≠mbolo | Significado |
|---------|-------------|
| üõ†Ô∏è ‚úÖ | Soporta tool calling, compatible con asistentes AI |
| üõ†Ô∏è ‚ùå | NO soporta tools |
| üëÅÔ∏è Visi√≥n | Puede analizar im√°genes |
| üíª C√≥digo | Capacidad de programaci√≥n |
| üåê Espa√±ol | Calidad de respuestas en espa√±ol |

#### ¬øQu√© modelo elegir?

| Tarea | Modelo Recomendado | VRAM | Tools |
|-------|-------------------|------|---------|  
| **Desarrollo general** | `glm4:9b-chat-q8_0` | ~9 GB | ‚úÖ |
| Modelo alternativo | `qwen2.5:7b` | ~5 GB | ‚úÖ |
| Modelo ligero/r√°pido | `qwen2.5:3b` | ~2 GB | ‚úÖ |
| C√≥digo especializado | `deepseek-coder:6.7b` | ~4 GB | ‚ùå |
| RAG / Embeddings | `nomic-embed-text` | ~500 MB | N/A |

> üí° `glm4:9b-chat-q8_0` es el modelo principal: GLM-4 cuantizado Q8 con excelente rendimiento en c√≥digo y razonamiento.
> üí° `qwen2.5:7b` como alternativa con excelente soporte en espa√±ol.### Instalaci√≥n de Modelos

```bash
# Instalar modelos requeridos
./scripts/setup.sh

# Instalar un modelo adicional manualmente
docker exec -it neuro-ollama ollama pull modelo:tag
```

## üîß Configuraci√≥n

### Variables de Entorno

| Variable | Default | Descripci√≥n |
|----------|---------|-------------|
| `OLLAMA_KEEP_ALIVE` | `-1` | Tiempo que el modelo permanece en memoria (-1 = indefinido) |
| `OLLAMA_CONTEXT_LENGTH` | `4096` | Longitud m√°xima del contexto |
| `OLLAMA_HOST` | `0.0.0.0` | Host donde escucha Ollama |

### GPU (NVIDIA)

El servicio est√° configurado para usar GPU NVIDIA por defecto. Requisitos:

1. **nvidia-container-toolkit** instalado:
   ```bash
   # Ubuntu/Debian
   sudo apt-get install nvidia-container-toolkit
   sudo systemctl restart docker
   ```

2. **Driver NVIDIA** compatible

Para ejecutar **sin GPU** (solo CPU), edita `docker-compose.yml` y comenta:
```yaml
# runtime: nvidia
# deploy:
#   resources:
#     reservations:
#       devices:
#         - driver: nvidia
#           count: all
#           capabilities: [gpu]
```

## üåê Conexi√≥n desde NEURO-OS

### Desarrollo Local

En `neuro-os/.vscode/tasks.json` o `dev.sh`:
```bash
OLLAMA_URL=http://localhost:11434
```

### Docker (mismo host)

En `neuro-os/docker-compose.yml`:
```yaml
environment:
  - OLLAMA_URL=http://host.docker.internal:11434
```

### Docker (host remoto)

```yaml
environment:
  - OLLAMA_URL=http://192.168.1.100:11434
```

## üîç Diagn√≥stico

```bash
# Verificar estado
./scripts/fix-ollama.sh

# Ver logs
docker compose logs -f

# Ver modelos instalados
curl http://localhost:11434/api/tags | jq

# Verificar GPU
docker exec -it neuro-ollama nvidia-smi
```

## üìÅ Estructura

```
neuro-ollama/
‚îú‚îÄ‚îÄ docker-compose.yml    # Configuraci√≥n del servicio
‚îú‚îÄ‚îÄ scripts/              # Scripts de administraci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ setup.sh          # Instalaci√≥n de modelos
‚îÇ   ‚îî‚îÄ‚îÄ fix-ollama.sh     # Diagn√≥stico/reparaci√≥n
‚îî‚îÄ‚îÄ README.md             # Este archivo
```

## üíæ Persistencia

Los modelos se almacenan en el volumen Docker `neuro-ollama-data`:

```bash
# Ver ubicaci√≥n del volumen
docker volume inspect neuro-ollama-data

# Backup de modelos (opcional)
docker run --rm -v neuro-ollama-data:/data -v $(pwd):/backup \
  alpine tar czf /backup/ollama-models.tar.gz /data
```

## üîÑ Actualizaci√≥n

```bash
# Actualizar imagen de Ollama
docker compose pull
docker compose up -d

# Los modelos se preservan en el volumen
```

## ‚ö†Ô∏è Troubleshooting

### Ollama no inicia

```bash
# Verificar logs
docker compose logs ollama

# Reiniciar
docker compose restart
```

### GPU no detectada

```bash
# Verificar nvidia-container-toolkit
nvidia-smi
docker run --rm --gpus all nvidia/cuda:12.0-base nvidia-smi

# Si falla, reinstalar toolkit
sudo apt-get install --reinstall nvidia-container-toolkit
sudo systemctl restart docker
```

### Modelos muy lentos

- Verificar que GPU est√° en uso: `docker exec -it neuro-ollama nvidia-smi`
- Reducir `OLLAMA_CONTEXT_LENGTH` si hay poca VRAM
- Usar modelos m√°s peque√±os (tier Light)

### Error de conexi√≥n desde neuro-os

1. Verificar que Ollama est√° corriendo: `curl http://localhost:11434/api/tags`
2. Verificar firewall: puerto 11434 debe estar abierto
3. En Docker, usar `host.docker.internal` en lugar de `localhost`

## üìÑ Licencia

MIT License - Ver archivo LICENSE para detalles.

<!-- AUTO-UPDATE-DATE -->
**√öltima actualizaci√≥n:** 2026-02-18 21:26:12 -03
